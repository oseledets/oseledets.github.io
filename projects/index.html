<!DOCTYPE html>
<html lang="en">
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta charset="utf-8"
>

        <title>Scientific Computing group </title>
        <meta name="author" content="scicomp">

        



<link rel="stylesheet" href="/theme/css/blueprint/src/reset.css" type="text/css" media="screen, projection">
<link rel="stylesheet" href="/theme/css/blueprint/src/liquid.css" type="text/css" media="screen, projection">
<link rel="stylesheet" href="/theme/css/blueprint/src/typography.css" type="text/css" media="screen, projection">
<link rel="stylesheet" href="/theme/css/blueprint/plugins/fancy-type/fancy-type.css" type="text/css" media="screen, projection">
<!--[if lt IE 8]>
  <link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection">
<![endif]-->
           <link rel="stylesheet" href="/theme/css/style.css" type="text/css">
        
        <link rel="stylesheet" href="/theme/css/zenburn.css" type="text/css">

    </head>

    <body>
        <div class="container">
<section id="main">
<div class="column span-17" >
<nav id="menu">
<ul class="menu">

    <li ><a href = "/">
            Home </a></li>

    <li ><a href = "/publications">
            Publications </a></li>

    <li  class = "active" ><a href = "/projects">
            Projects </a></li>

    <li ><a href = "/software">
            Software </a></li>
           </ul>
</nav> <!-- /#nav -->    <h1 id="student-research-projects-at-scientific-computing-group">Student research projects at Scientific Computing Group<a class="headerlink" href="#student-research-projects-at-scientific-computing-group" title="Permanent link">&para;</a></h1>
<p>Contact: Prof. Ivan Oseledets, i.oseledets@skoltech.ru, joint with the group&nbsp;members.</p>
<p>Below is the list of projects. 
Some of them can later evolve into the master theses. They can be also taken by a group of students&nbsp;(2-3). </p>
<h3 id="prerequisites">Prerequisites<a class="headerlink" href="#prerequisites" title="Permanent link">&para;</a></h3>
<p>The main prerequisite is the motivation work with <a href="oseledets.github.io">us</a>: our group is big, and we are always quite helpful, even if you are not a master in the particular topic. However it is a good idea that you know linear algebra (or at least learning it). For machine-learning related projects, machine learning experience is a good idea, for <span class="caps">PDE</span>-related it not bad  to have a basic knowledge about finite differences and/or finite elements. Many projects are related to <strong>tensor train</strong> and <strong>quantized tensor train</strong> decompositions: this is our &#8220;special feature&#8221;, which you will surely need to learn. Also, all the projects are related to programming and Python is the preferred&nbsp;language.</p>
<h3 id="tasks">Tasks<a class="headerlink" href="#tasks" title="Permanent link">&para;</a></h3>
<p>The tasks are of course specific for the particular projects, however the pipeline is&nbsp;similar.</p>
<ol>
<li>
<p>Do a literature survey. <a href="scholar.google.com">Google Scholar</a> will help you. Read all the papers you can find on the topic, not only the particular ones that the advisor has given to you. Follow the references in the papers, find keywords, define key&nbsp;ideas.</p>
</li>
<li>
<p>Find appropriate software and baseline algorithm. Typically, you are not the one solving a particular problem: look for the same projects, what was their approach and try to identify possible&nbsp;improvements</p>
</li>
<li>
<p>Start writing in <span class="math">\(\LaTeX\)</span> a short summary of what you are doing (something like a&nbsp;journal)</p>
</li>
<li>
<p>Try to formulate questions: a good question is the core to the&nbsp;answer.</p>
</li>
<li>
<p>Ask the questions to me or group&nbsp;members.</p>
</li>
<li>
<p>Write code, do experiments, get new results (or find that the approach is not working at all - this is also a scientific&nbsp;result).</p>
</li>
</ol>
<h3 id="list-of-projects">List of projects<a class="headerlink" href="#list-of-projects" title="Permanent link">&para;</a></h3>
<ol>
<li><a href="#deep-pde">Deep <span class="caps">PDE</span>:</a> Try to solve PDEs using deep&nbsp;learning</li>
<li><a href="#multi-classifier">Multi-classifier:</a> Learn many linear classifiers with low-rank tensor&nbsp;constraints</li>
<li><a href="#linear-classification">Linear classification:</a> Linear classification with many classes <span class="amp">&amp;</span> low-rank optimization: Given a machine learning task with huge number of classes, try to reduce the complexity by incorporating low-rank constraints into the&nbsp;problem.</li>
<li><a href="#maxvol-feature-selection">Maxvol feature selection:</a> Given machine learning task with many features, try to extract the &#8220;good&#8221; ones using maximum-volume&nbsp;algorithm </li>
<li><a href="#risk-analysis-and-high-dimensional-integrals">Risk analysis and high-dimensional integrals:</a> Using recently developed fast low-rank numerical quadrature technique, evaluate integrals over Brownian motion and apply&nbsp;to </li>
<li><a href="#sum-product-network-and-metropolis-algorithm">Sum-product-network and Metropolis algorithm:</a> Given a multivariate function, try to recover sum-product network for it based on Metropolis sampling&nbsp;method.</li>
<li><a href="#fast-direct-solver-for-sparse-matrices">Fast direct solver for sparse matrices:</a> Block elimination for hierarchically semiseparable&nbsp;matrices</li>
<li><a href="#boltzmann-collision-integral">Boltzmann collision integral:</a> Implement (possibly find a new one) fast technique for the evaluation of the Boltzmann collision&nbsp;integrals.</li>
<li><a href="#parallel-tensor-train-optimization">Parallel tensor-train optimization:</a> using message passing algorithm and its connection to loopy belief&nbsp;propagation.</li>
<li><a href="#finite-element-solver-for-topology-optimization-in-3d-printing">Finite element solver for topology optimization in 3D printing:</a>. Study different regularization techniques for the finite element (<span class="caps">FEM</span>) solvers for topology optimization of different&nbsp;shapes.</li>
<li><a href="#tt-cross-approximation-for-topology-optimization-in-3d-printing"><span class="caps">TT</span>-cross approximation for topology optimization in 3D printing:</a>. Given a &#8220;Lego-type structure&#8221; of a 3D topology, defined by a set of parameters, apply <span class="amp">&amp;</span> compute <span class="caps">TT</span>-cross approximation of the optimized&nbsp;quantity.</li>
<li><a href="#quantized-tensor-train-solver-topology-optimization-in-3d-printing">Quantized tensor train solver topology optimization in 3D printing:</a> Develop a <span class="caps">QTT</span> solver for shape <span class="amp">&amp;</span> topology optimization for different computational 3D printing&nbsp;tasks.</li>
<li><a href="#optimal-sparse-grids">Optimal sparse grids:</a> Given a set of multivariate functions, find the best interpolation points using maxvol&nbsp;algorithm.  </li>
<li><a href="#qttfun-project">QTTfun project:</a> Develop a <span class="caps">QTT</span> alternative to the Chebfun&nbsp;system.</li>
<li><a href="#tt-toolbox-production"><span class="caps">TT</span>-Toolbox production:</a> Develop a well-documented, clean codebase for the <span class="caps">TT</span>-Toolbox&nbsp;project.</li>
<li><a href="#tensors-on-gpu">Tensors on <span class="caps">GPU</span>:</a> Develop a prototype code for the <span class="caps">TT</span>-Toolbox on <span class="caps">GPU</span>&nbsp;(OpenCL).</li>
<li><a href="#tensors-on-mpi">Tensors on <span class="caps">MPI</span>:</a> Develop a prototype code for the <span class="caps">TT</span>-Toolbox on a parallel cluster (<span class="caps">MPI</span>).</li>
<li><a href="#volume-integral-equation-solvers-using-qtt">Volume integral equation solvers using <span class="caps">QTT</span>:</a> Given a volume integral equation, develop an efficient <span class="caps">QTT</span>-solver for&nbsp;it. </li>
<li><a href="#logistic-matrix-factorization-using-riemannian-optimization">Logistic matrix factorization using Riemannian optimization:</a> In many problems, instead of the Frobenious norm, other loss functionals are&nbsp;used.</li>
</ol>
<h2 id="deep-pde">Deep <span class="caps">PDE</span><a class="headerlink" href="#deep-pde" title="Permanent link">&para;</a></h2>
<p>Deep learning is now extremely popular for many different tasks, including image processing, speech recognition and many others.
On the other hand, PDEs require a lot of computing  power. The goal of this project is to study the applicability of deep learning methods
(based on standard software) to approximate the functionals of solution of&nbsp;PDEs.</p>
<h3 id="problem-description">Problem description.<a class="headerlink" href="#problem-description" title="Permanent link">&para;</a></h3>
<p>Consider a thermal conductivity&nbsp;problem
</p>
<div class="math">$$
   \nabla \cdot k \nabla u = f,
$$</div>
<p>
where <span class="math">\(k\)</span> is a given coefficient. Then we want to approximate the functional <span class="math">\(F(u)\)</span> of the solution (i.e., maximal temperature, mean temperature).
The goal is to design a neural network that will be able to approximate this quantity to sufficient&nbsp;accuracy.</p>
<h3 id="tasks_1">Tasks.<a class="headerlink" href="#tasks_1" title="Permanent link">&para;</a></h3>
<ul>
<li>Learn how to solve thermal conductivity problem&nbsp;numerically</li>
<li>Create a database of solutions/values of the&nbsp;functional</li>
<li>Design a deep architecture capable of approximating the&nbsp;functional.</li>
</ul>
<h2 id="multi-classifier">Multi-classifier<a class="headerlink" href="#multi-classifier" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_1">Problem description.<a class="headerlink" href="#problem-description_1" title="Permanent link">&para;</a></h3>
<p>We propose the following classifier. Given a feature vector <span class="math">\(p\)</span>, the
classification is performed by the selection of the path
<span class="math">\(i_1, \ldots, i_d\)</span> that maximizes the &#8220;softmax&#8221; criteria [
    F(p, i_1, \ldots, i_d) = \frac{p W_1(i_1) \ldots W_d(i_d)}{ p W_1(i_1) \ldots W_{d-1}(i_{d-1}) W}, 
] where <span class="math">\(W_d = \sum_{i_d = 1}^c W_d(i_d)\)</span> and the last variable <span class="math">\(i_d\)</span>
is the class variable (which we want to discover). The learning of the
matrices <span class="math">\(W_k(i_k)\)</span> can be done by a gradient-descent type algorithm:
for each training sample <span class="math">\((p_s, c_s)\)</span> given <span class="math">\(W\)</span> we update the best path
<span class="math">\(i^{(s)}_1, \ldots, i^{(s)}_{d-1}\)</span> by maximizing the classification, and
then for a given set <span class="math">\((p_s, i_1, \ldots, i_d)\)</span> we minimize the loss (for
example, logistic regression on the softmax criteria, or just the
softmax itself) by taking a Riemanian gradient&nbsp;descent.</p>
<p>This can be viewed as a \textbf{multiple linear classificator}, but with
a special structure of the classificator matrices, otherwise the
overfitting would be obvious. For small number of layers the inference
the classification would not be hard (i.e. <span class="math">\(10\)</span> layers each of size
<span class="math">\(2\)</span>).</p>
<p>It is also close to the <a href="http://arxiv.org/abs/1505.00387">highway networks</a>.</p>
<h3 id="tasks_2">Tasks.<a class="headerlink" href="#tasks_2" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with logistic&nbsp;regression</li>
<li>Get familiar with Riemannian&nbsp;optimization</li>
<li>Get familar with <a href="http://github.com/oseledets/ttpy"><span class="caps">TT</span>-Toolbox</a></li>
<li>Implement the multi-classifier&nbsp;approach </li>
</ol>
<h2 id="linear-classification">Linear classification<a class="headerlink" href="#linear-classification" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_2">Problem description.<a class="headerlink" href="#problem-description_2" title="Permanent link">&para;</a></h3>
<p>A typical linear regression uses the following classifier. Given a feature vector <span class="math">\(x\)</span> and number of classes <span class="math">\(c\)</span>, we learn an <span class="math">\(F \times C\)</span> weight matrix <span class="math">\(W\)</span> and the probabilities for different classes are given by the&nbsp;vector
</p>
<div class="math">$$
  p = \frac{1}{Z} \exp(W x), 
$$</div>
<p>
where <span class="math">\(Z\)</span> is the normalization constant. For large number of features and large number of classes (maybe infinite, when the class variable is continious), storing matrix <span class="math">\(W\)</span> is difficult, and we propose to constraint it to the rank-<span class="math">\(r\)</span>&nbsp;case.</p>
<h3 id="tasks_3">Tasks.<a class="headerlink" href="#tasks_3" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with rank-<span class="math">\(r\)</span>&nbsp;matrices</li>
<li>Get familiar with Riemannian optimization over rank-<span class="math">\(r\)</span>&nbsp;manifolds</li>
<li>Find a good problem with many&nbsp;classes</li>
<li>Test the&nbsp;method!</li>
</ol>
<h2 id="maxvol-feature-selection">Maxvol feature selection<a class="headerlink" href="#maxvol-feature-selection" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_3">Problem description.<a class="headerlink" href="#problem-description_3" title="Permanent link">&para;</a></h3>
<p>A machine learning task with many features can be difficult to solve; however, many features are not so important, and only important features can be used for learning larger and more accurate classifiers. But how to select those features?
The idea that we want to test is based on the <strong>maximal volume submatrices</strong>. Represent the 
data as <span class="math">\(N_{obj} \times N_F\)</span>, and find there the submatrix with <strong>maximal determinant</strong>. </p>
<h3 id="tasks_4">Tasks.<a class="headerlink" href="#tasks_4" title="Permanent link">&para;</a></h3>
<ol>
<li>Look for different feature selection algorithms in the&nbsp;literature</li>
<li>Find appropriate many-features&nbsp;datasets</li>
<li>Implement the maximal volume submatrix approach (see, for example,&nbsp;https://bitbucket.org/muxas/rect_maxvol)</li>
</ol>
<h2 id="risk-analysis-and-high-dimensional-integrals">Risk analysis and high-dimensional integrals<a class="headerlink" href="#risk-analysis-and-high-dimensional-integrals" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_4">Problem description.<a class="headerlink" href="#problem-description_4" title="Permanent link">&para;</a></h3>
<p>Many stock prices can modelled using <strong>Brownian motion</strong>, i.e. the next state depends on the previous state plus some random event. Important statistical options (for example,  option calls) can be then estimated as expectations of random process. The straightforward way is to do Monte-Carlo simulation, but it can not give high accuracy. Another way is to write down the expectation as a path integral and then approximate it by a multi-dimensional integral. For similar integrals we have recently developed a new technique, http://arxiv.org/abs/1504.06149.
The idea is to test it for examples from risk&nbsp;analysis.</p>
<h3 id="tasks_5">Tasks.<a class="headerlink" href="#tasks_5" title="Permanent link">&para;</a></h3>
<ol>
<li>Read the papers on high-dimensional integrals in risk&nbsp;analysis</li>
<li>Read the paper&nbsp;http://arxiv.org/abs/1504.06149</li>
<li>Implement the code from http://arxiv.org/abs/1504.06149 for high-dimensional financial&nbsp;integrals</li>
</ol>
<h2 id="sum-product-network-and-metropolis-algorithm">Sum-product-network and Metropolis algorithm<a class="headerlink" href="#sum-product-network-and-metropolis-algorithm" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_5">Problem description.<a class="headerlink" href="#problem-description_5" title="Permanent link">&para;</a></h3>
<p><a href="http://homes.cs.washington.edu/~pedrod/papers/uai11a.pdf">Sum-product-networks</a> is a new deep architecture. One of its interesting property is that there a constructive algorithm for learning the structure of such network from the observations. On the other hand, one can look at <span class="caps">SPN</span> as an approximation of multivariate functions. The idea is then to use Metropolis-type algorithm that samples the given function as it was the probability density, and then apply the <span class="caps">SPN</span> learning to recover the <span class="caps">SPN</span>&nbsp;structure. </p>
<h3 id="tasks_6">Tasks.<a class="headerlink" href="#tasks_6" title="Permanent link">&para;</a></h3>
<ol>
<li>Read literature about&nbsp;sum-product-networks</li>
<li>Get familiar with software that learns the network&nbsp;structure</li>
<li>Get familiar with the Metropolis sampling&nbsp;algorithm</li>
<li>Try to compute <span class="caps">SPN</span> for deterministic&nbsp;functions</li>
</ol>
<h2 id="fast-direct-solver-for-sparse-matrices">Fast direct solver for sparse matrices<a class="headerlink" href="#fast-direct-solver-for-sparse-matrices" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_6">Problem description.<a class="headerlink" href="#problem-description_6" title="Permanent link">&para;</a></h3>
<p>Sparse matrices are important and have <span class="math">\(\mathcal{O}(N)\)</span> parameters. However solving a linear system is much more difficult. The goal of this project is to get familiar with the subject, and implement the fast direct solver based on so-called <span class="caps">HSS</span>&nbsp;structure. </p>
<h3 id="tasks_7">Tasks.<a class="headerlink" href="#tasks_7" title="Permanent link">&para;</a></h3>
<ol>
<li>Read literature on fast low-rank direct&nbsp;solvers </li>
<li>Study our&nbsp;code</li>
<li>Implement a variant for <span class="caps">HSS</span>&nbsp;structure</li>
</ol>
<h2 id="boltzmann-collision-integral">Boltzmann collision integral<a class="headerlink" href="#boltzmann-collision-integral" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_7">Problem description.<a class="headerlink" href="#problem-description_7" title="Permanent link">&para;</a></h3>
<p>Boltzmann collission integral is an integral transform that takes two functions from <span class="math">\(L^2(\mathbb{R}^3)\)</span> to a one function in <span class="math">\(L^2(\mathbb{R}^3)\)</span>. For example, see http://www.sam.math.ethz.ch/sam_reports/reports_final/reports2012/2012-28.pdf.
Discretizated on a grid, it can be defined as a <span class="math">\(N^3 \times N^3 \times N^3\)</span> array, and evaluation requires <span class="math">\(\mathcal{O}(N^9)\)</span> complexity. However, it has many symmetries. Our goal is to compare different method for the evaluation of the Boltzmann&nbsp;integral.</p>
<h3 id="tasks_8">Tasks.<a class="headerlink" href="#tasks_8" title="Permanent link">&para;</a></h3>
<ol>
<li>Read the literature for numerical evaluation of Boltzmann collision&nbsp;integrals </li>
<li>Come up with the discretization&nbsp;scheme</li>
<li>Implement it and evaluate its&nbsp;efficiency</li>
</ol>
<h2 id="parallel-tensor-train-optimization">Parallel tensor-train optimization<a class="headerlink" href="#parallel-tensor-train-optimization" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_8">Problem description.<a class="headerlink" href="#problem-description_8" title="Permanent link">&para;</a></h3>
<p><span class="caps">TT</span>-format (Tensor Train) is a representation of <span class="math">\(d\)</span>-dimensional tensors of the&nbsp;form 
</p>
<div class="math">$$A(i_1, \ldots, i_d) = G_1(i_1) \ldots G_d(i_d),$$</div>
<p>
where <span class="math">\(G_k(i_k)\)</span> has size <span class="math">\(r_{k-1} \times r_k\)</span> for fixed <span class="math">\(i_k\)</span>. A typical task to find the <span class="caps">TT</span>-representation is to optimize over the cores <span class="math">\(G_k\)</span> in a sequential manner (all fixed, update one and so on). Our goal is to present an asynchonous versions of such algorithm, based on the connection with <strong>loopy belief&nbsp;propagation</strong></p>
<h3 id="tasks_9">Tasks.<a class="headerlink" href="#tasks_9" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with tensor&nbsp;trains</li>
<li>Get familiar with loopy belief&nbsp;propagation</li>
<li>Implement an asynchonous optimization for <span class="caps">TT</span>-fromat and study its&nbsp;efficiency.</li>
</ol>
<h2 id="finite-element-solver-for-topology-optimization-in-3d-printing">Finite element solver for topology optimization in 3D printing<a class="headerlink" href="#finite-element-solver-for-topology-optimization-in-3d-printing" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_9">Problem description.<a class="headerlink" href="#problem-description_9" title="Permanent link">&para;</a></h3>
<p>The &#8220;mega-goal&#8221; is given a functional <span class="math">\(F\)</span> find a topology that optimizes this functional given some constaints (weight of the structure, for example).  One of the typical task is linear elasticity, and it is being solved by iteration over the material properties. One can formulate the problem as a minimization problem with constraints&nbsp;(regularization). </p>
<h3 id="tasks_10">Tasks.<a class="headerlink" href="#tasks_10" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with <span class="caps">FEM</span> approaches for topology&nbsp;optimization</li>
<li>Select an appropriate topology optimization&nbsp;problem</li>
<li>Formulate it as an inverse problem with a suitable&nbsp;regularization</li>
<li>Implement a prototype&nbsp;code</li>
<li>(Bonus) Print the&nbsp;structure</li>
</ol>
<h2 id="tt-cross-approximation-for-topology-optimization-in-3d-printing"><span class="caps">TT</span>-cross approximation for topology optimization in 3D printing<a class="headerlink" href="#tt-cross-approximation-for-topology-optimization-in-3d-printing" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_10">Problem description.<a class="headerlink" href="#problem-description_10" title="Permanent link">&para;</a></h3>
<p>The &#8220;mega-goal&#8221; is given a functional <span class="math">\(F\)</span> find a topology that optimizes this functional given some constaints (for example, weight  of the structure). One of the ways to do so is to use &#8220;ball and sticks&#8221; method to create the structure: a set of primitives that can be connected in different ways and also parametrized with several parameters. Checking all possible parameters has exponential complexity, and each computation requires a solution of a <span class="caps">PDE</span>. The idea is to use <span class="caps">TT</span>-cross method to adaptively sample the parameter&nbsp;space.</p>
<h3 id="tasks_11">Tasks.<a class="headerlink" href="#tasks_11" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with <span class="caps">FEM</span> approaches for topology&nbsp;optimization</li>
<li>Select a suitable local model, determine the number of&nbsp;parameters</li>
<li>Get familar with <span class="caps">TT</span>-cross approximation&nbsp;algorithm</li>
<li>Write a prototype&nbsp;code</li>
</ol>
<h2 id="quantized-tensor-train-solver-topology-optimization-in-3d-printing">Quantized tensor train solver topology optimization in 3D printing<a class="headerlink" href="#quantized-tensor-train-solver-topology-optimization-in-3d-printing" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_11">Problem description.<a class="headerlink" href="#problem-description_11" title="Permanent link">&para;</a></h3>
<p>The &#8220;mega-goal&#8221; is given a functional <span class="math">\(F\)</span> find a topology that optimizes this functional given some constaints (weights).  One of the typical task is linear elasticity, and it is being solved by iteration over the material properties. One can formulate the problem as a minimization problem with constraints (regularization). 
Our goal here is to use the weirdest regularization: contraint the material properties to low-<span class="caps">QTT</span> rank case. However, you can be smarter, and come up with other ideas, for example coming from deep networks as well (that would allow for very curved shapes to be generated). However to work on this project you should get familiar both with the <span class="caps">FEM</span> solvers for a topology optimization codes, and with optimization with low-rank&nbsp;constraints.</p>
<h3 id="tasks_12">Tasks.<a class="headerlink" href="#tasks_12" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with <span class="caps">FEM</span> approaches for topology&nbsp;optimization</li>
<li>Get familiar with Tensor-Train&nbsp;optimization</li>
<li>Try to apply <span class="caps">TT</span>/<span class="caps">QTT</span> contraints to the material coefficients to get new optimized&nbsp;shapes.</li>
</ol>
<h2 id="optimal-sparse-grids">Optimal sparse grids<a class="headerlink" href="#optimal-sparse-grids" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_12">Problem description.<a class="headerlink" href="#problem-description_12" title="Permanent link">&para;</a></h3>
<p>Approximation of multivariate functions is a notoriously difficult tasks. One of the most efficient approaches, based on so-called <strong>sparse grids</strong> is based on using the set polynomials with bounded <strong>total degree</strong>. This provides a sparse basis set in the set of multidimensional functions. Interpolation is the simplest task, and in the standard setting gives rise to the so-called <strong>sparse grids</strong>. However, given the basis, the problem of selecting optimal interpolation points can be solved by the <strong>maximum volume algorithm</strong>=. The goal of this project is to compute those interpolation sets and evaluate their&nbsp;efficiency.</p>
<h3 id="tasks_13">Tasks.<a class="headerlink" href="#tasks_13" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with sparse&nbsp;grids</li>
<li>Get familiar with max-vol&nbsp;algorithm</li>
<li>Implement a functional version of the maxvol&nbsp;algorithm</li>
<li>Estimate the efficiency of the&nbsp;approach</li>
</ol>
<h2 id="qttfun-project">QTTfun project<a class="headerlink" href="#qttfun-project" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_13">Problem description.<a class="headerlink" href="#problem-description_13" title="Permanent link">&para;</a></h3>
<p><span class="caps">QTT</span> (Quantized Tensor Train) is a very elegant <span class="amp">&amp;</span> efficient representation of functions, which in many cases allows to achieve complexity logarithmic in the number of samples. However, even for 1d-functions using the basic software (http://github.com/oseledets/ttpy) it is not that straightforward to compute integrals, find roots. This is all implemented in the Chebfun system in a very nice and lucrative way. The goal of this project is implement a <span class="caps">QTT</span> alternative to the Chebfun project, and it should be much more&nbsp;efficient. </p>
<h3 id="tasks_14">Tasks.<a class="headerlink" href="#tasks_14" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with tensor train/<span class="caps">QTT</span>. Install the basic&nbsp;software</li>
<li>Get familiar with Chebfun&nbsp;system</li>
<li>Implement QTTFun 0.1 with basic overloaded functions (arithmetics, integrals,&nbsp;etc.)</li>
</ol>
<h2 id="tt-toolbox-production"><span class="caps">TT</span>-Toolbox production<a class="headerlink" href="#tt-toolbox-production" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_14">Problem description.<a class="headerlink" href="#problem-description_14" title="Permanent link">&para;</a></h3>
<p><span class="caps">TT</span>-Toolbox (http://github.com/oseledets/ttpy), there is also a <span class="caps">MATLAB</span> version, is our basic software for working with tensors, and it is actively used in many groups around the world. However it is not <strong>well-documented</strong> and the number of examples is limited. The codebase should be also cleaned as&nbsp;well.</p>
<h3 id="tasks_15">Tasks.<a class="headerlink" href="#tasks_15" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with tensor train&nbsp;Toolbox </li>
<li>Implement basic&nbsp;examples</li>
<li>Try to document the code, find bugs, propose new&nbsp;functionality</li>
</ol>
<h2 id="tensors-on-gpu">Tensors on <span class="caps">GPU</span><a class="headerlink" href="#tensors-on-gpu" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_15">Problem description.<a class="headerlink" href="#problem-description_15" title="Permanent link">&para;</a></h3>
<p>Tensor train is a very efficient representation of multidimensional tensors. However, there are use-cases when not only an asymptotic complexity is needed, but the actual computation speed matters. Our main application in mind is machine learning with low-rank constraints, when someone has to do many iterations, and if it is possible to speedup the computations on <span class="caps">GPU</span>, it also improved the results. The <span class="caps">TT</span>-Toolbox (http://github.com/oseledets/ttpy) is implemented in Python with no <span class="caps">GPU</span> support. The goal of this project is two-fold: get familiar with modern <span class="caps">GPU</span> programming techniques, including pyopencl and Loo.py and based upon those, build a <span class="caps">GPU</span> version of basic tensor&nbsp;algorithms</p>
<h3 id="tasks_16">Tasks.<a class="headerlink" href="#tasks_16" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with tensor train&nbsp;Toolbox</li>
<li>Get familiar with <span class="caps">GPU</span> programming with pyopencl and&nbsp;Loo.py</li>
<li>Implement a prototype code for basic <span class="caps">TT</span> algorithms on <span class="caps">GPU</span>, evaluate their performance compared to the peak&nbsp;one</li>
</ol>
<h2 id="tensors-on-mpi">Tensors on <span class="caps">MPI</span><a class="headerlink" href="#tensors-on-mpi" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_16">Problem description.<a class="headerlink" href="#problem-description_16" title="Permanent link">&para;</a></h3>
<p>Tensor train is a very efficient representation of multidimensional tensors. There are use-cases, when the representation is so large that is does not fit into the memory (i.e., large <span class="caps">TT</span>-ranks) and parallelization is required.  The goal of this project is to try to figure out how the algorithms implemented in the <span class="caps">TT</span>-Toolbox (http://github.com/oseledets/ttpy) can be parallelized (for example, using mpi4py) and test the resulting prototype. Moreover,the actual parametrization of <span class="caps">TT</span> is very sequential, and it would be really interesting to consider asyncronous approaches to <span class="caps">TT</span>&nbsp;optimization. </p>
<h3 id="tasks_17">Tasks.<a class="headerlink" href="#tasks_17" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with tensor train&nbsp;Toolbox</li>
<li>Get familiar with <span class="caps">MPI</span> programming using&nbsp;mpi4py</li>
<li>Implement a prototype code for basis <span class="caps">TT</span>-algorithms using <span class="caps">MPI</span>, evaluate their&nbsp;performance</li>
</ol>
<h2 id="volume-integral-equation-solvers-using-qtt">Volume integral equation solvers using <span class="caps">QTT</span><a class="headerlink" href="#volume-integral-equation-solvers-using-qtt" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_17">Problem description.<a class="headerlink" href="#problem-description_17" title="Permanent link">&para;</a></h3>
<p>Probably you have some understanding, what a partial differential equation is. However, there are cases, when <strong>volume integral equations</strong> (i.e., the system has the form <span class="math">\(K u = f\)</span>, and <span class="math">\(K\)</span> is a non-local operator, for&nbsp;example,
</p>
<div class="math">$$
   \int \frac{q(y) dy}{\Vert x - y \Vert} = f.
$$</div>
<p>
After discretization, we are left with <span class="math">\(N^3 \times N^3\)</span> dense matrix. For a uniform grid, the complexity is then greatly reduced by using Fast Fourier Transform (<span class="caps">FFT</span>) to <span class="math">\(\mathcal{O}(N^3 \log N)\)</span> for one matrix-by-vector product. 
The goal of this project is to reduce this complexity much further, to <span class="math">\(\mathcal{O}(N)\)</span> or even <span class="math">\(\mathcal{O}(\log^{\alpha}(N)\)</span> using <strong>quantized tensor train</strong>&nbsp;representation.</p>
<h3 id="tasks_18">Tasks.<a class="headerlink" href="#tasks_18" title="Permanent link">&para;</a></h3>
<ol>
<li>Get familiar with volume integral&nbsp;equations</li>
<li>Pick one interesting <span class="caps">VIE</span> (probably with some application in&nbsp;mind)</li>
<li>Get familiar with Tensor Train/ <span class="caps">QTT</span>&nbsp;formats</li>
<li>Implement a basic solver for <span class="caps">VIE</span> using <span class="caps">TT</span>/<span class="caps">QTT</span> formats using <span class="caps">TT</span>-Toolbox</li>
</ol>
<h2 id="logistic-matrix-factorization-using-riemannian-optimization">Logistic matrix factorization using Riemannian optimization<a class="headerlink" href="#logistic-matrix-factorization-using-riemannian-optimization" title="Permanent link">&para;</a></h2>
<h3 id="problem-description_18">Problem description.<a class="headerlink" href="#problem-description_18" title="Permanent link">&para;</a></h3>
<p>Low-rank matrix factorization of the&nbsp;form
</p>
<div class="math">$$
   A \approx U V^{\top}, 
$$</div>
<p>
where <span class="math">\(U\)</span> is <span class="math">\(n \times r\)</span> and <span class="math">\(V\)</span> is <span class="math">\(m \times r\)</span>, appears in many applications, for example in recommender systems, where the rows enumerate the users, and the columns - the product. However, the typical norm used is the Frobenius norm, and it actually does not fit very well: we are interested only in the prediction of like/dislike, but not the actual numerical value. Thus, the optimization should be formulated in a non-standard way. Logistic matrix factorization is one of the possible ways to do so. Optimization of non-quadratic functionals can be done using Riemannian optimization in a very efficient way. This is the goal of this&nbsp;project.</p>
<h3 id="tasks_19">Tasks.<a class="headerlink" href="#tasks_19" title="Permanent link">&para;</a></h3>
<ol>
<li>Understand where the logistic matrix factorization comes&nbsp;from</li>
<li>Get familiar with basic Riemannian optimization&nbsp;techniques</li>
<li>Implement a Riemannian optimization approach for logistic matrix&nbsp;factorization</li>
<li>Compare with existing software on standard&nbsp;examples.</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>
            </section>

<div class="column prepend-1 span-6" >
    <style type="text/css">
        .bottom-three {
          padding: 8px 15px;
          color: #06c;
          font-size: 200%;
          text-decoration: underline;
          margin-bottom: 20px;
       }
       td.good-table {
               padding: 2px 4px 10px 2px;
               text-align: left;
           }
       th.good-table {
               padding: 2px 4px 10px 2px;
           }
       table.good-table {
               padding: 2px 4px 10px 2px;
           }
       tr.good-table {
               padding: 2px 4px 10px 2px;
           }
       </style>

       <link rel="stylesheet" href="/theme/css/blueprint/src/reset.css" type="text/css" media="screen, projection">
       <link rel="stylesheet" href="/theme/css/blueprint/src/liquid.css" type="text/css" media="screen, projection">
       <link rel="stylesheet" href="/theme/css/blueprint/src/typography.css" type="text/css" media="screen, projection">
       <link rel="stylesheet" href="/theme/css/blueprint/plugins/fancy-type/fancy-type.css" type="text/css" media="screen, projection">
       <!--[if lt IE 8]>
         <link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection">
         <![endif]-->
                    <link rel="stylesheet" href="/theme/css/style.css" type="text/css">
                            
                            <link rel="stylesheet" href="/theme/css/zenburn.css" type="text/css">

<p class = "bottom-three" ><a href='/news'>News</a></p>
<table>
    <thead>
    </thead>
<tbody>
<tr> 
    <td>
        26/05/2016
    </td>
    <td><a href='/news/a-tt-eigenvalue-solver-that-finally-works/' >A <span class="caps">TT</span>-eigenvalue solver that finally&nbsp;works</a>
    </td>
    <td>
    </td>
</tr>   
<tr> 
    <td>
        12/05/2016
    </td>
    <td><a href='/news/exponential-machines-and-tensor-trains/' >Exponential machines and tensor&nbsp;trains</a>
    </td>
    <td>
    </td>
</tr>   
<tr> 
    <td>
        06/04/2016
    </td>
    <td><a href='/news/convergence-analysis-of-a-projected-fixed-point-iteration/' >Convergence analysis of a projected fixed-point&nbsp;iteration</a>
    </td>
    <td>
    </td>
</tr>   
<tr> 
    <td>
        30/03/2016
    </td>
    <td><a href='/news/compress-and-eliminate-solver-for-sparse-matrices/' >Compress-and-eliminate solver for sparse&nbsp;matrices</a>
    </td>
    <td>
    </td>
</tr>   
<tr> 
    <td>
        01/12/2015
    </td>
    <td><a href='/news/new-paper-in-simax/' >New paper in <span class="caps">SIMAX</span></a>
    </td>
    <td>
    </td>
</tr>   
</tbody>
</table>

<h2> Contact </h2>
<p>
We are located at the 2-nd floor of the new "Technopark-3” building in Skolkovo (few kilometers outside Moscow Ring Road). 
The building is accessible from Skolkovo Road (Сколковское шоссе) and Minskoe Highway (Минское шоссе).
</p>
<p> email: <img src="/images/email.png"></p> 

<p> </p>

</div>
    </div>
<section id="footer"> 
<div class = "column span-24">
    <p style="text-align:center">
Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
</p>
</div>
</section>

    </body>
</html>
